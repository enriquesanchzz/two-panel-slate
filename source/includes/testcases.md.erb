# Test Cases 
Jeeves Projects contain **Test Cases**. You can create Test Cases to examine the availability and throughput of a website or web application which enables you to look for for features to improve points or bugs to fix. 

This section contains you the following information about Test Cases:

* [Test Case Presets](#test-case-presets)
* [Create a Test Case](#create-a-test-case)
* [Test Case Settings](#test-case-settings)
  * [Run Parameters](#run-parameters)
  * [Health Checks](#health-checks)
  * [Charts and Indicators](#charts-and-indicators)
  * [Rename a Test Case](#rename-a-test-case)
  * [Delete a Test Case](#delete-a-test-case)
* [Test Case Home Page](#test-case-home-page)
  * [Run a Test Case](#run-a-test-case)
  * [Test Case Logs](#test-case-logs)
  * [Test Case Insights](#test-case-insights)
  * [Test Case Reports](#test-case-reports)


## Test Case Presets
When you start to [create a Test Case](#create-a-test-case), Jeeves prompts you to choose a Preset. A **Preset** is a type of prefabricated Test Case configuration, which you can modify to fit your Projects testing needs. 

The following table lists the Test Case Presets available in Jeeves:

| Preset | Description |
| ------ | ----------- |
| **Get a Single Url** | Tests the response time to get a single URL for a fixed RPS number. |
| **Get a Single Url Changing a Header Value** | Tests the response time to get a single URL for a fixed RPS number changing a header value. |
| **Get a Single Url Changing a Path Parameter** | Tests the response time to get a single URL for a fixed RPS number changing a path parameter. |
| **Get a Single Url Changing a Query Value** | Tests the response time to get a single URL for a fixed RPS number changing a query value. |
| **Get a Single Url from Different Hosts** | Tests the response time to get a single URL for a fixed RPS number from different hosts. |
| **Post to a Single Url** | Tests the response time to post to  a single URL for a fixed RPS number. |
| **Post to a Single Url Changing a Query Value** | Tests the response time to post to  a single URL for a fixed RPS number changing a query value.|
| **Copy the Configuration from a Different Test Case** | Creates a Test Case copying the configuration of the Test Case name given. |

To learn how to create a Test Case, see the [Create a Test Case](#create-a-test-case) section.

<aside class="warning"> To know more about the specific configuration parameters of each Preset, see the <a href="#preset">Preset</a> subsection of the <a href="#run-parameters">Run Parameters</a> section.
</aside>

<a href="#test-cases" class="go-back"> ← Go back to the Test Cases section </a>



## Create a Test Case
To create a new **Test Case** for a project profile, follow these steps:

1. Select a project from the **Jeeves WebApp** home page.
</br>The selected project home page appears.
2. Select the **Test Cases** tab.
3. Click the **+ Create** button in the top left corner of the **Test Cases** tab.
</br>The **New Test Case** page appears.
4. Enter the name of your new Test Case in the **Name** field.
5. Select a **Preset** type for your new Test Case.
6. Click the **Ok** button.
</br>A new empty **Test Case Home page** appears.

<aside class="success"> After following the preceding steps, you have created a Test Case successfully.
</aside>

To learn more about how to run and configure Test Cases, see the [Test Case Home Page](#test-case-home-page) section.

<a href="#test-cases" class="go-back"> ← Go back to the Test Cases section </a>



## Test Case Home Page
Test Cases have a **Home page** where you can view the information obtained from the previous Test Case runs, or configure your Test Case to fit your testing needs. The Test Case Home page shows the following sections: 

* [Test Case Settings](#test-case-settings)
* [Run a Test Case](#run-a-test-case)
* [Test Case Insights](#test-case-insights)
* [Test Case Logs](#test-case-logs)
* [Test Case Reports](#test-case-reports)

After creating a new Test Case, the Test Case information sections are empty and the Test Case is set up with default configuration, which you need to edit to be able to test your Project.  
To learn how to configure your Test Case, see the [Test Case Settings](#test-case-settings) section.

<a href="#test-cases" class="go-back"> ← Go back to the Test Cases section </a>



### Test Case Settings
Test Cases are configured with default settings when created. To test your web page or web application, you must adapt this configuration to your Test Case setting. 

To access a **Test Case Settings**, follow these steps:

1. Select a Test Case from the selected **Project** home page.
2. Click the name of the selected Test Case.
</br>The selected Test Case home page appears.
3. Click the **Testcase Settings** button on the top right corner of the **Test Case Home page**.
</br>The **Test Case settings** page appears.

The **Test Case Settings** page contains the following tabs, each one containing parameters and settings that can be adapted to your Project Test Case settings. The following sections show information about the configuration settings on the following tabs: 
 
* [Run Parameters](#run-parameters)
* [Health Checks](#health-checks)
* [Charts and Indicators](#charts-and-indicators)
* [Rename a Test Case](#rename-a-test-case)
* [Delete a Test Case](#delete-a-test-case)

<a href="#test-cases-home-page" class="go-back"> ← Go back to the Test Cases Home Page section </a>



#### Run Parameters
The **Run Parameters** configuration settings define the way a Test Case runs. The Run Parameters are JSON files that store the information that defines a Test Case, like the target [URL](#url) or the number of [requests per second (RPS)](#config).

The Run Parameters settings tab can present the following sections, depending of the Preset of the Test Case:

* [Preset](#preset)
* [Config](#config)
* [Run Agent](#run-agent)
* [URL](#url)
* [Headers](#headers)
* [Expected Status](#expected-status)
* [Dynamic Run Parameters](#dynamic-run-parameters)

To know about the specific parameters for each Test Case Preset, see the [Preset](#preset) section.

<a href="#test-case-settings" class="go-back"> ← Go back to the Test Cases Settings section </a>



#### Preset
A **Preset** is the type of configuration that you selected when creating your Test Case. The Preset contains the prefabricated configuration of a Test Case to be modified to fit your testing needs. 

Depending on the type of Preset you select, some parameters may vary. The following table lists the Presets and their available configurations: 

| Preset | Available Configurations |
| ------ | ------------------------ |
| **Get a Single Url** | Config |
| | Url |
| | Headers |
| | Expected Status |
| **Get a Single Url Changing a Header Value** | Config |
| | Url |
| | Dynamic Header Key |
| | Dynamic Header Values |
| | Default Headers |
| | Expected Status |
| **Get a Single Url Changing a Path Parameter** | Config |
| | Url |
| | Dynamic Values |
| | Headers |
| | Expected Status |
| **Get a Single Url Changing a Query Value** | Config |
| | Url |
| | Dynamic Values |
| | Headers |
| | Expected Status |
| **Get a Single Url from Different Hosts** | Config |
| | Dynamic Hosts |
| | Path |
| | Headers |
| | Expected Status |
| **Post to a Single Url** | Config |
| | Url |
| | Dynamic Body |
| | Headers |
| | Expected Status |
| **Post to a Single Url Changing a Query Value** | Config |
| | Url |
| | Dynamic Values |
| | Body |
| | Headers |
| | Expected Status |

To change the **Preset** type of your Test Case,  follow these steps:

1. Click the **Preset** section of the **Run Parameters** tab.
</br>A dropdown menu with the Preset options appears.
2. Select the new **Preset** configuration for your Test Case.
3. Click the **Ok** Button.
</br>Jeeves loads the default settings of the new Preset.

<aside class="success">After following the preceding steps, you have changed your Test Case Preset successfully. 
</aside>

To learn more about the Run Agent configuration of your Test Case, see the [Run Agent](#run-agent) section.

#### Run Agent
The **Run Agent** section of the Run Parameters shows the configuration for the Test Case Run Agent. The Run Agent configuration appears when working with any environment other than localhost. 

The following table lists the available parameters of the Run Agent section:

| Parameter | Description | Type | Notes |
| --------- | ----------- | ---- | ----- |
| **Peers** | The number of AWS Spot Instances in which the Test Case runs.  | Integer | Each Peer runs as a separate node with its own workload, but results are grouped. |
| **Warmup Url** | The URL to test connectivity to the Test Case target host. | String | The Warmup Url and the Target URL of the Test Case must belong to the same host. In case the Warmup Url is not reachable, the Test Case doesn’t run. |
| **Spot Size** | The size and characteristics configuration of the Spot Instance. | String | For further information on the Spot Instances sizes configuration, see the [Amazon EC2 Instance Types](https://aws.amazon.com/ec2/instance-types/) page by Amazon. |

To edit the **Run Agent** parameters, follow these steps:

1. Select the **Run Agent** tab in the **Run Parameters** page.
</br>A page with the **Run Agent** parameters fields appears.
2. Enter the number of peers in the **Peers** field.
3. Enter the Warm up URL to test connectivity to the host in the **Warmup Url** field. 
4. Select the type of Spot Instance from the **Spot Size** dropdown list.
5. Click the **Ok** button to save your changes.
</br>Your Run Agent parameters are set up.

<aside class="success"> After following the preceding steps, you have set up your Run Agent settings successfully.
</aside>

To learn more about the Run Parameters configuration of your Test Case, see the [Config](#config) section.

<a href="#run-parameters" class="go-back"> ← Go back to the Run Parameters section </a>



#### Config
The **Config** section of the Run Parameters shows the JSON file that contains the environment parameters for the Test Case to be replicated. Each Test Case requires a set of parameters that vary depending on the Test Case Preset.
The following table lists the available parameters of the Config JSON file:

 
| Parameters | Description | Type | Notes |
| ---------- | ----------- | ---- | ----- |
| **rps (Request Per Second)** | The number of request the Test Case does to target URL | Integer | Valid Values: 1 to 360,000 |
| **rampup** | The speed at which new concurrent requests try to access the target URL | Integer | Valid Values: 0 to 7,200 |
| **duration** | The time in seconds to complete the Test Case run | Integer | Valid Values: 5 to 7,200 |
| **strategy** | The form in which the data samples are selected | String | Valid Values: Circular Random |
| | | | In Circular strategy, each peer start in index 0 |
| **followRedirects** | The boolean that defines if the target URL will load the following redirect resource | Boolean | Default value: false |



The following script is an example of a Config parameters JSON file:

``` json
[
  {
    "rps": 10,
    "rampup": 30,
    "duration": 50,
    "followRedirects": false,
    "strategy": "random"
  }
]
```

To edit the **Config** parameters, follow these steps:

1. Select the **Config** tab in the **Run Parameters** page.
2. Select the **Form Edit** option tab.
</br>A page with the **Config** field appears.
3. Enter the RPS  for your Test Case in the **Rps** field.
4. Enter the seconds to reach the target RPS for your Test Case in the **Rampup** field.
5. Enter the seconds to complete your Test Case in the **Duration** field.
6. If applicable, select the data selection Strategy for your test case with the **Circular** or **Random** button.
7. If applicable, define if your Test Case loads the following redirect source with the **Follow Redirects** switch.
8. Click the **Ok** button to save your changes.
</br>Your Config settings are set up.

<aside class="success">After following the preceding steps, you have set up your Config settings successfully.
</aside>

To know how to set up a Test Case URL, see the [URL](#url) section.

<a href="#run-parameters" class="go-back"> ← Go back to the Run Parameters section </a>



#### URL
The **URL** section of the Run Parameters shows the JSON file with the Target URL for the Test Case. The target URL defines the web address of the project being tested. The following table lists the available parameters of the URL JSON file:


| Parameter | Description | Type | Notes |
| --------- | ----------- | ---- | ----- |
| **url** | The Target URL for the Test Case | String | To learn how to add Path or Query Parameters, see the [Dynamic Run Parameters](#dynamic-run-parameters) section. |



The following script  is an example of a Target URL JSON file:

```json
[
  {
    "url":"https://petstore.octoperf.com/actions/Catalog.action"
  }
]
```

For more information on Target URLs using Path or Query values, see the [Dynamic Run Parameters](#dynamic-run-parameters) section.

To edit the **Target URL**, follow these steps:

1. Select the **URL** tab in the **Run Parameters** page.
2. Select the **Form Edit** option tab.
</br>A page with the **Url** field appears.
3. Enter the URL for your Test Case in the **Url** field.
4. Click the **Ok** button to save your changes.
</br>Your Config settings are set up.

<aside class="success">After following the preceding steps, you have set up your Target URL successfully.
</aside>

To know how to set up the headers of a Target URL, see the [Headers](#headers) section.

<a href="#run-parameters" class="go-back"> ← Go back to the Run Parameters section </a>



#### Headers
The **Headers** section of the Run Parameters shows the JSON file with the HTTP Headers needed to run the Test Case. Headers let Jeeves and the web server pass additional information with an HTTP Request or Response. The following table lists the parameters of the Headers JSON file:

| Parameter | Description | Type | Notes |
| --------- | ----------- | ---- | ----- |
| **key** | The name of the default HTTP Header of the Target URL. | String | For further information on Headers, see the [HTTP Header](https://developer.cdn.mozilla.net/en-US/docs/Web/HTTP/Headers) list by MDN Web Docs. |
| **value** | The value that the default HTTP Header receives in the Target URL server. | String | |
 


 The following script is an example of a Headers JSON file:

 ```json
[
  {
    "key": "Accept",
    "value": "*/*"
  },
  {
    "key": "Accept-Language",
    "value": "en-US,en;q=0.5"
  }
]
```

To edit the **Headers** list, follow these steps:

1. Select the **Headers** tab in the **Run Parameters** page.
2. Select the **Form Edit** option tab.
</br>A page with the **Key** and **Value** fields appears.
3. Edit the name of your Default Header in the **Key** field.
4. Edit the Value of your Default Header in the **Value** field:
</br>a. Click the **+** plus button to add a new **Default Header**.
</br>b. Repeat steps 3 and 4.
</br>c. Repeat step 4.a if necessary.
5. Click the **Ok** button to save your changes.
</br>Your Headers are set up.

<aside class="success">After following the preceding steps, you have set up your Headers successfully.
</aside>

To know how to set up a Test Case Expected Status, see the [Expected Status](#expected-status) section.

<a href="#run-parameters" class="go-back"> ← Go back to the Run Parameters section </a>



#### Expected Status
The **Expected Status** section of the Run Parameters shows the JSON file with the expected Status Code Responses after each request run. The following table lists the available parameters of the Expected Status JSON file:

| Parameter | Description | Type | Notes |
| --------- | ----------- | ---- | ----- |
| **status**  | The number of the HTTP Status Code expected after the Test Case run. | String | For further information on Status Codes see the [HTTP Status Code]("https://developer.mozilla.org/en-US/docs/Web/HTTP/Status") list by MDN Web Docs. |


 The following script is an example of an Expected Statuses JSON file:

 ```json
[
  {
    "status": 200
  },
  {
    "status": 201
  },
  {
    "status": 202
  }
]
```

For further information on the meaning of the status codes, see the [HTTP Response Status Codes]("https://developer.mozilla.org/en-US/docs/Web/HTTP/Status") page by the MDN Web Docs.

To edit the **Expected Status** JSON file, follow these steps:

1. Select the **Expected Status** tab in the **Run Parameters** page.
2. Select the **Form Edit** option tab.
</br>A page with the **Status** field appears.
3. Modify the value of your Status codes:
</br>a. Click the **+** plus button to add a new **Expected Status**.
</br>b. Repeat the previous step if necessary.
4. Click the **Ok** button to save your changes.
</br>Your Expected Status codes are now set up.

  
<aside class="success">After following the preceding steps, you have set up your Expected Status codes successfully.
</aside>

To know how to set up Dynamic Run Parameters in Test Cases that change a parameter, see the [Dynamic Run Parameters](#dynamic-run-parameters) section.

<a href="#run-parameters" class="go-back"> ← Go back to the Run Parameters section </a>



#### Dynamic Run Parameters
Test Cases created with the following Presets have **Dynamic Run Parameters** to configure: 

* Get a Single Url Changing a Path Parameter
* Get a Single Url Changing a Query Value
* Post to a Single Url Changing a Header Value
* Get a Single Url from Different Hosts
* Post to a Single Url

The **Dynamic Run Parameters** section shows the JSON files with the Path, Query, Header, Host, or Body parameters to change to the target URL when running a Test Case. To use Dynamic Values, you must configure the following depending of your Test Case [Preset](#preset):

* [Path and Query Parameters](#path-and-query-parameters)
  * [Dynamic Target URL](#dynamic-target-url)
  * [Dynamic Values](#dynamic-values)
* [Header Parameters](#header-parameters)
  * [Dynamic Key Header](#dynamic-key-header)
  * [Dynamic Header Values](#dynamic-header-values)
  * [Default Headers](#default-headers)
* [Host Parameters](#host-parameters)
  * [Dynamic Hosts](#dynamic-hosts)
  * [Path](#path)
* [Body Parameter](#body-parameter)

<a href="#run-parameters" class="go-back"> ← Go back to the Run Parameters section </a>


##### Path and Query Parameters
The **Path and Query parameters** enable Jeeves to change a part of the Target URL to access sections of the Web page or Web App to test with a Test Case.  Path and Query parameters are defined in the URL, but they have different structure and objective:

* **Path parameters:** Enable developers to identify a specific item from a resource. 
  
  ```
  # Get the user with the id 123
  https://mypage.com/users/123 
  ```

* **Query parameters:** Enable developers to filter or sort items from a resource.
  
  ```
  # Get all users with the occupation "technical writer"
  www.mypage.com/users?occupation=technical-writer 
  ```

To learn how to change Path and Query parameters to the Target URL, see the [Dynamic Target URL](#dynamic-target-url) section.

<a href="#dynamic-run-parameters" class="go-back"> ← Go back to the Dynamic Run Parameters section </a>


###### Dynamic Target URL
Jeeves uses String Interpolation to change the Path or Query parameters in the **Dynamic Target URL** with the values defined in the Dynamic Values JSON File. To achieve this, Jeeves defines the path and query parameters on the Target URL by using the following structure:

This is an example of a Path parameter:

```
/${path-parameter}/
```

This is an example of a Query parameter:

```
?query-parameter=${query-value}
```

This is an example of the Target URL with a Path parameter:

```
https://en.wikipedia.org/wiki/The_Wall_Street_Journal
```

This is an example of the Target URL with a Query parameter:

```
https://en.wikipedia.org/wiki/Special:RecentChanges?hidebots=1
```

To set a **Dynamic Target URL**, follow these steps:

1. Select the **Url** tab in the **Run Parameters** page.
2. Select the **Form Edit** or **Raw Edit** option tab.
3. Edit your Target URL like the following example including your Path or Query parameters:
</br>`https://mypage.com/${path-parameter}?query-parameter=${query-value}`
4. Click the **Ok** button.
</br>Your Dynamic Target URL is set up.

<aside class="success">After following the preceding steps, you have set up your Dynamic Target URL successfully.
</aside>

To define the Dynamic Values JSON file, see the [Dynamic Values](#dynamic-values) section.

<a href="#dynamic-run-parameters" class="go-back"> ← Go back to the Dynamic Run Parameters section </a>



###### Dynamic Values
Once you set up your [Dynamic Target URL](#dynamic-target-url), you must define the values of the Path and Query parameters of your Dynamic Target URL so the Test Case can run the testing correctly. 

The following script is an example of a  Dynamic Values JSON file for the example Target URL:

Example Target URL:

`https://professional.dowjones.com/${page}`

Dynamic values JSON file:

```json[
  {
    "key": "page",
    "value": "risk"
  },
  {
    "key": "page",
    "value": "factiva"
  },
  {
    "key": "page",
    "value": "newswires"
  }
]
```

The example above contains a Target URL with one [Path parameter](#path-and-query-parameters), and the Dynamic Values JSON file with the three values for the path parameter of the Target URL. 

To set up the **Dynamic Values** for a Test Case, follow these steps:

1. Select the **Dynamic Values** tab in the **Run Parameters** page
2. Select the **Form Edit** option tab
</br>A page with the **Value** field appears
3. Edit the value of your Path Parameter in the **Value** field
</br>a. Click the **+** plus button to add a new Dynamic Value
</br>b. Repeat the previous step if necessary
4. Click the **Ok** button to save your changes
</br>Your Dynamic Values are set up

<aside class="success">After following the preceding steps, you have set up your Dynamic Values successfully.
</aside>

To know how to set up your Test Cases to use Dynamic Headers, see the [Header Parameters](#header-parameters) section.

<a href="#dynamic-run-parameters" class="go-back"> ← Go back to the Dynamic Run Parameters section </a>



###### Header Parameters
Test Cases created with the  Get a Single Url Changing a Header Value Preset test the functionality of a web page by changing a header value. Therefore, the headers of the web page become dynamic because of this value change. The **Dynamic Headers** settings available for the Get a Single Url Changing a Header Value are the following:

* [Dynamic Key Header](#dynamic-key-header)
* [Dynamic Header Values](#dynamic-header-values)
* [Default Headers](#default-headers)

<a href="#dynamic-run-parameters" class="go-back"> ← Go back to the Dynamic Run Parameters section </a>

##### Dynamic Key Header
The **Dynamic Key Header** section shows the key header that receives the [Dynamic Header Values](#dynamic-header-values) during the Test Case run. Test Cases are limited to one Dynamic Key Header. The following table lists the parameters of the Dynamic Key Header JSON file:

| Parameter | Description | Type | Notes |
| --------- | ----------- | ---- | ----- |
| **key** | The name of the Dynamic key HTTP Header of the Target URL. | String | For further information on Headers, see the [HTTP Header]("https://developer.cdn.mozilla.net/en-US/docs/Web/HTTP/Headers") list by MDN Web Docs.

The following script is an example of a Dynamic Key Header JSON file:

```json
[
  {
    "key": "Accept"
  }
]
```

For further information on the meaning of the Key Headers, see the [HTTP Headers]("https://developer.cdn.mozilla.net/en-US/docs/Web/HTTP/Headers") list page by the MDN Web Docs.

To change the **Dynamic Key Header**, follow these steps:

1. Select the **Dynamic Key Header** tab in the **Run Parameters** page.
2. Select the **Form Edit** option tab.
</br>A page with the **Key** field appears.
3. Edit the value of your Key Header in the **Key** field.
4. Click the **Ok** button to save your changes.
</br>Your Dynamic Key Header is set up.

<aside class="success">After following the preceding steps, you have set up your Dynamic Key Header successfully.
</aside>

To learn how to set up the dynamic values of your Dynamic Key Header, see the [Dynamic Header Values](#dynamic-header-values) section.

<a href="#dynamic-run-parameters" class="go-back"> ← Go back to the Dynamic Run Parameters section </a>



###### Dynamic Header Values
The Dynamic Header Values section shows the JSON file with the values to set up for the Dynamic Key Header. The following table lists the parameters of the Dynamic Header Values JSON file:

| Parameter | Description | Type | Notes |
| --------- | ----------- | ---- | ----- |
| **value** | The value that the Dynamic Key HTTP Header receives in the Target URL server. | String | For further information on Headers, see the [HTTP Header]("https://developer.cdn.mozilla.net/en-US/docs/Web/HTTP/Headers") list by MDN Web Docs.
 
The following script is an example of a Dynamic Header Values JSON file:

```json
[
  {
    "value": "application/json"
  },
  {
    "value": "image/jpeg"
  }
]
```

For further information about the accepted values for your required Key Header, see the [HTTP Headers]("https://developer.cdn.mozilla.net/en-US/docs/Web/HTTP/Headers") list page by the MDN Web Docs.

To change the **Dynamic Header Values**, follow these steps:

1. Select the **Dynamic Header Values** tab in the **Run Parameters** page.
2. Select the **Form Edit** option tab.
</br>A page with the **Value** field appears.
3. Edit the value of your Dynamic Header Value in the **Value** field.
4. Click the **Ok** button to save your changes.
</br>Your Dynamic Header Values are set up.

<aside class="success">After following the preceding steps, you have set up your Dynamic Header Values successfully.
</aside>

To learn how to set up the default headers for your Test Case, see the [Default Headers](#default-headers) section.

<a href="#dynamic-run-parameters" class="go-back"> ← Go back to the Dynamic Run Parameters section </a>



###### Default Headers
The **Default Headers** section shows the JSON file with the HTTP Headers needed to run the Test Case. Headers let the Jeeves and the web server pass additional information with an HTTP Request or Response. The following table lists the parameters of the Default Headers JSON file:

| Parameter | Description | Type | Notes |
| --------- | ----------- | ---- | ----- |
| **key** | The name of the default HTTP Header of the Target URL. | String | For further information on Headers, see the HTTP Heade list by MDN Web Docs. |
| **value** | The value that the default HTTP Header receives in the Target URL server. | String |


 The following script is an example of a Default Headers JSON file:

 ```json
[
  {
    "key": "Accept",
    "value": "*/*"
  },
  {
    "key": "Accept-Language",
    "value": "en-US,en;q=0.5"
  }
]
```

To change the **Default Headers**, follow these steps:

1. Select the **Default Headers** tab in the **Run Parameters** page.
2. Select the **Form Edit** option tab.
</br>A page with the **Key** and **Value** fields appears.
3. Edit the name of your Default Header in the **Key** field.
4. Edit the Value of your Default Header in the **Value** field.
5. If neccesary, Click the **+** plus button to add a new Default Header.
</br>a. Repeat steps 3 and 4 .
</br>b. Repeat step 4.a if necessary.
6. Click the **Ok** button to save your changes.
</br>Your Default Headers are set up.

<aside class="success">After following the preceding steps, you have set up your Default Headers successfully.
</aside>

To set up dynamic host parameters for your Test Case, see the [Host Parameters](#host-parameters) section.

<a href="#dynamic-run-parameters" class="go-back"> ← Go back to the Dynamic Run Parameters section </a>



##### Host Parameters
Test Cases created with the **Get a Single Url From Different Hosts** test the accessibility and speed of a web page to be loaded from different hosts. This means that the Test Cases test a Target URL located at different servers around the world. The **Dynamic Host** settings available for the **Get a Single Url From Different Hosts** are the following:

* [Dynamic Host](#dynamic-hosts)
* [Path](#path)

<a href="#dynamic-run-parameters" class="go-back"> ← Go back to the Dynamic Run Parameters section </a>


###### Dynamic Hosts
The **Dynamic Hosts** section of the Run Parameters shows the JSON file with the list of hosts from where the Test Case tries to test the Target URL accessibility and load speed. The following table lists the available parameters for the Dynamic Host JSON file:


| Parameter | Description | Type |
| --------- | ----------- | ---- |
| **name** | The name of the host to test  Target URL. | String |
| **host** | The Target URL for the host to test | String |

The following script is an example of a Dynamic Hosts JSON file:

```json
[
  {
    "name": "Wikipedia English",
    "host": "https://en.wikipedia.org"
  },
  {
    "name": "Wikipedia Spanish",
    "host": "https://es.wikipedia.org"
  },
  {
    "name": "Wikipedia Italian",
    "host": "https://it.wikipedia.org"
  }
]
```

To edit the **Dynamic Host** list, follow these steps:

1. Select the **Dynamic Host** tab in the **Run Parameters** page.
2. Select the **Form Edit** option tab.
</br>A page with the **Name** and **Host** fields appears.
3. Edit the name of your Dynamic Host in the **Name** field.
4. Edit the Dynamic Host URL in the **Host** field.
    </br>a. Click the **+** plus button to add a new Dynamic Host.
    </br>b. Repeat steps 3 and 4 .
    </br>c. Repeat step 4.a if necessary.
5. Click the **Ok** button to save your changes.
</br>Your Dynamic Hosts are set up.

<aside class="success">After following the preceding steps, you have set up your Dynamic Hosts successfully.
</aside>

To learn how to set up the path for the Dynamic Host of a Test Case, see the [Path](#path) section.

<a href="#dynamic-run-parameters" class="go-back"> ← Go back to the Dynamic Run Parameters section </a>



###### Path
The **Path** section of the Run Parameters shows the JSON file with the target section of the Target URL the Test Case tests in every Dynamic Host. The following table lists the available parameters for the Path JSON file:


| Parameter | Description | Type |
| --------- | ----------- | ---- |
| **path** | The query and path parameters that complement the Target URL to test in the Dynamic Hosts. | String |

The following Script shows an example of a Path JSON file:

```json
[
  {
    "path": "/wiki/Special:Random"
  }
]
```

To change the **Path**, follow these steps:

1. Select the **Path** tab in the **Run Parameters** page.
2. Select the **Form Edit** option tab.
</br>A page with the **Path** field appears.
3. Enter the Path values for your Dynamic Hosts in the **Path** field.
4. Click the **Ok** button to save your changes.
</br>Your Path is set up.

<aside class="success">After following the preceding steps, you have set up your Path successfully.
</aside>

To set up dynamic host parameters for your Test Case, see the [Body Parameter](#body-parameter) section.

<a href="#dynamic-run-parameters" class="go-back"> ← Go back to the Dynamic Run Parameters section </a>



##### Body Parameter
Test Cases created with the following Presets have a **Body Parameter**:

* Post to a Single URL
* Post to a Single URL Changing a Query Parameter

The **Body Parameter** helps the Test Cases with the preceding Presets to Post values to a Target URL. Since each URL has different parameters, JSON files in this section may vary from Project to Project. 

To edit the **Test Case Body** parameter, follow these steps:

1. Select the **Body** tab in the **Run Parameters** page.
2. Click the **Edit Form** tab.
3. Click the **Open the File Explorer** button.
</br>A File explorer window appears.
4. Select the JSON file with the body of your POST.
5. Click the **Open** button.
</br>The name of the file appears in the **Submit your file** section.
6. Click the **Ok** button.
</br>The JSON file with your POST body is updated.

<aside class="success"> After following the preceding steps, you have set up your Test Case Body successfully.
</aside>

After you finish configuring the Run Parameters settings, you can further configure your Test Case to help you define:

* [Health Checks](#health-checks)
* [Charts and Indicators](#charts-and-indicators)

See the sections listed above for further information. 

<a href="#dynamic-run-parameters" class="go-back"> ← Go back to the Dynamic Run Parameters section </a>



#### Health Checks
**Health Checks** are analytical algorithms that diagnose the run results and trends of a Test Case. It helps to understand the results of a Test Case run by presenting graphical representations of the results with graphs and icons. Three types of health checks exist, each one representing an aspect of the run results:

* [Average Response Time](#average-response-time)
* [Succeeded vs Failed](#succeeded-vs-failed)
* [Worst Response Time](#worst-response-time)

See the sections listed above for further information. 

<a href="#test-case-settings" class="go-back"> ← Go back to the Test Case Settings section </a>


##### Average Response Time
The **Average Response Time** health check shows the time each request took to be completed. The health check represents the results through a linear graph and it shows a **BAD** red sign if the Average Response Time is higher or equal to the specified time. 

<aside class="notice"> By default, the Average Response Time health check is set up to 800 microseconds(MS).
</aside>

The Average Response Time graph and sign appear like the following image:

<img src="images/avg-response-time.png" alt="Average Response Time chart">

To change the expected **Average Response Time** for a Test Case run, follow these steps:

1. Select the **Health Checks** tab of the **Testcase Settings** page.
</br>The Health Checks page appears.
2. Select how to measure the Average Response Time:
</br>a. Change the Average Response Time expected microseconds (**MS**) result field, or
</br>b. Change the **times** each response time increments with respect to the  Average Response Time obtained in the previous Test Case run.
3. Click the **Ok** button in the bottom right corner of the page.
</br>Your Average Response Time health check is updated.

<aside class="success"> After following the preceding steps, you have set up your Average Response Time health check successfully.
</aside>

<a href="#test-case-settings" class="go-back"> ← Go back to the Test Case Settings section </a>



##### Succeeded vs Failed
The **Succeeded vs Failed** health check shows the percentage of requests that failed or succeeded to obtain a response. The health check represents the results through a bar graph and shows a **BAD** red sign if the percentage of requests that failed to obtain a result is higher or equal to the specified percentage.

<aside class="notice"> By default, the Succeeded vs Failed health check percentage is set up to %50.
</aside>

The Succeeded vs Failed graph and sign appear like the following image: 

<img src="images/succ-failed.png" alt="Succeeded vs Failed chart">

To change the expected **Succeeded vs Failed** percentage for a Test Case run, follow these steps:

1. Select the **Health Checks** tab of the **Testcase Settings** page.
</br>The **Health Checks** page appears.
2. Select how to measure the Succeeded vs Failed percentage.
</br>a. Change the Succeeded vs Failed expected percentage (**%**)  result field, or
</br>b. Change the Succeeded vs Failed **percentage** increment of the run compared to the Succeeded vs Failed percentage obtained in the previous Test Case run.
3. Click the **Ok** button in the bottom right corner of the page.
</br>Your Succeeded vs Failed health check is updated.

<aside class="success"> After following the preceding steps, you have set up your Succeeded vs Failed health check successfully.
</aside>

<a href="#test-case-settings" class="go-back"> ← Go back to the Test Case Settings section </a>



##### Worst Response Time
The **Worst Response Time** health check shows the best and worst time taken for a request to obtain a response. The health check represents the results through a bar graph and shows a **BAD** red sign if the worst time to obtain a result is higher or equal to the specified time.

<aside class="notice"> By default, the Worst Time Response health check is set up to 1200 microseconds (MS).
</aside>

The Worst Time Response graph and sign appear like the following image:

<img src="images/worst-response-time.png" alt="Worst Response Time chart">

To change the expected **Worst Response Time** for a Test Case run, follow these steps:

1. Select the **Health Checks** tab of the **Testcase Settings** page.
</br>The **Health Checks** page appears.
2. Select how to measure the Worst Response Time.
</br>a. Change the Worst Response Time expected microseconds (**MS**)  result field, or
</br>b. Change the **times** each response time increments with respect to the  Average Response Time obtained in the previous Test Case run.
3. Click the **Ok** button in the bottom right corner of the page.
</br>Your Worst Response Time health check is updated.

<aside class="success"> After following the preceding steps, you set up your Worst Response Time health check successfully.
</aside>

<a href="#test-case-settings" class="go-back"> ← Go back to the Test Case Settings section </a>



#### Charts and indicators
The **Charts and Indicators** section shows the parameters that define the times in which a Test Case considers a request response is fast, slow, or fails to complete. The following list contains the Chart and Indicators parameters for the Test Cases:

* **Response Time Lower Bound:** The lowest expected time in which a request gets a response.
* **Response Time Higher Bound:** The highest expected time in which a request gets a response.
* **Response Timeout:** The time in which a request is close because it did not get a response. 


To know how to interpret the charts created with these parameters, see the [Test Case Insights](#test-case-insights) and the [Test Case Reports](#test-case-reports) sections.

<a href="#test-case-settings" class="go-back"> ← Go back to the Test Case Settings section </a>


#### Rename a Test Case
To **rename a Test Case**, follow these steps:

1. Click on the **Testcase Settings** button on the top right corner of the page.
</br>The **Testcase Settings** page appears.
2. Select the **Rename Testcase** tab.
</br>A **Name** field appears.
3. Enter the new name for the Test Case in the **Name** field.
4. Click the **Ok** button.
</br>The Testcase Home page displays the new name on the top left corner of the Testcase Home page.

<aside class="success"> After following the preceding steps, you have changed the name of the Test Case successfully.
</aside>

<a href="#test-case-settings" class="go-back"> ← Go back to the Test Case Settings section </a>



#### Delete a Test Case
To **delete a Test Case**, follow these steps:

1. Click on the **Testcase Settings** button on the top right corner of the page.
</br>The **Testcase Settings** page appears.
2. Select the **Other Settings** tab.
</br>The **Delete this testcase** radio button appears.
3. Enable the **Delete this testcase** radio button
4. Click the **Ok** button.
</br>The Testcase is deleted and the Project Home page appears with the remaining Test Cases. 

<aside class="success"> After following the preceding steps, you have deleted the Test Case successfully.
</aside>

<a href="#test-case-settings" class="go-back"> ← Go back to the Test Case Settings section </a>



### Run a Test Case
Once you set up your Test Case with the configuration of your Web page or Web App, you are able to run your Test Case. To **run a Test Case**, click the **Run** button below the name of the Test Case.

<img src="images/run-button.png" alt="Run Test Case button">

To **stop a Test Case**, click the **Stop** button below the name of the Test Case. 

<img src="images/stop-button.png" alt="Stop Test Case button">

After you run a Test Case for the first time, the Test Case Home page presents you the following sections with information about the last 20 runs of the Test Case:

* [Test Case Logs](#test-case-logs)
* [Test Case Insights](#test-case-insights)
* [Test Case Reports](#test-case-reports)

To learn how to interpret the information of each section, see the sections above.

<a href="#test-case-home-page" class="go-back"> ← Go back to the Test Case Home Page section </a>


### Test Case Logs
When you run a Test Case, the **Logs** section of your Test Case Home page shows you the activities in progress in real-time. You can check the Logs after the run is finished. 

See the sections of a Log in the following list to understand how to read Logs:

* [Initializing](#initializing)
* [Running](#running)
* [Finalizing](#finalizing)
* [Delivering](#delivering)

<a href="#test-case-home-page" class="go-back"> ← Go back to the Test Case Home Page section </a>



#### Initializing
When you run a Test Case, Jeeves starts the following three activities:

1. **Execute Run Command:** Jeeves starts the Run command. The Run command launches a series of background activities to create the Simulation Work of the Test Case.
2. **Create the Simulation Work:** Jeeves creates the Simulation Work for the Test Case. Jeeves takes the Project and Test Case parameters to create the Simulation Work.
3. **Running the Simulation Work:** Jeeves runs the Simulation Work created in the preceding step. Once the Simulation Work is running without problems, the Test Case iterations start running.

The following script is an example of how the Initializing section appears in the Logs:

```
[RUN AGENT] Joining the pending queue...   # 1 second after launch
[RUN AGENT] Executing run command...   # 2 seconds after launch
[RUN AGENT] Run command successfully executed
Tue Jun 01 17:37:43 UTC 2021 [SIMULATION WORKER] Creating simulation work...
Tue Jun 01 17:37:45 UTC 2021 [SIMULATION WORKER] 
      project = dow-jones
      testcase = path-parameter
      preset = GetASingleUrlChangingAPathParameter   
      total peers = 1
      peer number = 0
      
Tue Jun 01 17:37:45 UTC 2021 [SIMULATION WORKER] Loading configuration...
Tue Jun 01 17:37:47 UTC 2021 [SIMULATION WORKER] Running simulation...
Tue Jun 01 17:37:48 UTC 2021 [SIMULATION WORKER] [SIMULATION WORKER] Still alive
Simulation GetASingleUrlChangingAPathParameter started...
Tue Jun 01 17:37:53 UTC 2021 [SIMULATION WORKER] [SIMULATION WORKER] Still alive
```

To understand how Logs report the running simulation, see the [Running](#running) section.

<a href="#test-case-logs" class="go-back"> ← Go back to the Test Case Logs section </a>


#### Running
Once Jeeves created the Simulation Work with the Project and Test Case parameters, the Test Case Simulation Work starts. Jeeves creates reports with the Test Case run health every five seconds. The reports include the following information:

* **Timestamp:** The Timestamp shows the time when the log is registered, along with the seconds that have passed since the beginning of the simulation. 
* **Request Results:**  The Request Results shows the aspects the Test Case is testing on the Target URL, along with the count of how many requests have succeeded **(OK)** and how many failed **(KO)**.
* **Test Case Preset:** The Test Case Preset shows the name of the Test Case Preset.
* **Simulation Status:** The Simulation Status shows the percentage of requested completed along with the following classification:
  * **waiting:** The amount of requests waiting to be served by the Simulation Work.
  * **active:** The amount of requests being served by the Simulation Work at the moment.
  * **done:** The amount of requests that have been served by the Simulation Work until this moment.

The following script is an example of how the Running section appears in the Logs:

```
================================================================================
2021-06-01 17:38:00                                          10s elapsed
---- Requests ------------------------------------------------------------------
> Global                                                   (OK=14     KO=0     )
> newswires                                                (OK=4      KO=0     )
> factiva                                                  (OK=3      KO=0     )
> risk                                                     (OK=7      KO=0     )

---- GetASingleUrlChangingAPathParameter ---------------------------------------
[##-------------                                                           ]  2%
          waiting: 400    / active: 86     / done: 14    
================================================================================
```


to understand how Logs report the ending of the running simulation, see the [Finalizing](#finalizing) section.

<a href="#test-case-logs" class="go-back"> ← Go back to the Test Case Logs section </a>



#### Finalizing
Once Jeeves completed the total amount of requests in the Simulation Work, the Log presents the results obtained. The following list shows the information registered in this section of the Log, where the parameters show the Total Amount of Request per category, along with the Amount of Succeeded Request (OK) and the Amount of Failed Request (KO) per category:

* **Time taken to complete:** the time that the Simulation Work took to complete the requests. 
* **request count:** The amount of  requests the Simulation Work took.
* **Min response time:** The lowest response time a request got.
* **Max response time:** The highest response time a request got.
* **Mean response time:** The total response time divided by the number of requests.
* **Std deviation:** The amount of dispersion of values from the mean response time.
* **Response time 50th percentile:** The amount of requests served when the Simulation Work is at 50% progress time.
* **Response time 75th percentile:** The amount of requests served when the Simulation Work is at 75% progress time.
* **Response time 95th percentile:** The amount of requests served when the Simulation Work is at 95% progress time.
* **Response time 99th percentile:** The amount of requests served when the Simulation Work is at 99% progress time.
* **Mean request/sec:** The average time a request takes to complete.
* **T < response time lower bound in ms:** The amount of requests completed under the given Response Time Lower Bound.
* **response time lower bound in ms < T < response time higher bound in ms:** The amount of request completed between the given Response Time Lower Bound and the Response Time Higher Bound.
* **T > response time higher bound in ms:** The amount of request completed over the given Response Time higher Bound.
* **Failed:** The amount of request that failed to be completed. 

To know more about the Response Time Bounds, see the [Charts and Indicators](#charts-and-indicators) section.

The following script is an example of how the Finalizing section appears in the Logs: 

```
Simulation GetASingleUrlChangingAPathParameter completed in 50 seconds
Parsing log file(s)...
Parsing log file(s) done
Generating reports...

================================================================================
---- Global Information --------------------------------------------------------
> request count                                        340 (OK=340    KO=0     )
> min response time                                    373 (OK=373    KO=-     )
> max response time                                   6174 (OK=6174   KO=-     )
> mean response time                                  1513 (OK=1513   KO=-     )
> std deviation                                       1102 (OK=1102   KO=-     )
> response time 50th percentile                       1139 (OK=1139   KO=-     )
> response time 75th percentile                       2277 (OK=2277   KO=-     )
> response time 95th percentile                       3705 (OK=3705   KO=-     )
> response time 99th percentile                       4593 (OK=4593   KO=-     )
> mean requests/sec                                    6.8 (OK=6.8    KO=-     )
---- Response Time Distribution ------------------------------------------------
> t < 800 ms                                           155 ( 46%)
> 800 ms < t < 1200 ms                                  16 (  5%)
> t > 1200 ms                                          169 ( 50%)
> failed                                                 0 (  0%)
================================================================================
```

To understand how logs report the creation of Test Case Reports, see the [Delivering](#delivering) section.

<a href="#test-case-logs" class="go-back"> ← Go back to the Test Case Logs section </a>



#### Delivering
Once the Simulation Work delivers the results in the Finalizing section of the Log, the Simulation Work finishes the Test Case run by creating the Test Case Reports. This last section of the logs explains that the report about the Test Case run is created and then finalizes the Simulation Work. 

The following script is an example of how the Delivering  section appears in the Logs: 

```
Tue Jun 01 17:38:41 UTC 2021 [SIMULATION WORKER] Finished simulation
Tue Jun 01 17:38:41 UTC 2021 [SIMULATION WORKER] Calling on success task...
Tue Jun 01 17:38:41 UTC 2021 [SIMULATION WORKER] Saving report...
Tue Jun 01 17:38:42 UTC 2021 [SIMULATION WORKER] Saving statistics...
Tue Jun 01 17:38:42 UTC 2021 [SIMULATION WORKER] Finished on success task
Tue Jun 01 17:38:42 UTC 2021 [SIMULATION WORKER] Terminated
SLF4J: The requested version 1.7.16 by your slf4j binding is not compatible with [1.6]
SLF4J: See http://www.slf4j.org/codes.html#version_mismatch for further details.
[LOGGING WORKER] The process terminated
[LOGGING WORKER] Calling on termination task...
[RUN AGENT] The simulation took 1 min 0 sec
[RUN AGENT] Saving log...
```

To learn how to interpret Test Case run charts and reports, see the [Test Case Insights](#test-case-insights) and the [Test Case Reports](#test-case-reports) sections.

<a href="#test-case-logs" class="go-back"> ← Go back to the Test Case Logs section </a>



### Test Case Insights
When you run a Test Case, the **Insight** section of your Test Case Home page shows you two sections with charts: 

* **Last Run:** The Last Run chart shows the results of the amount of requests run. The chart consist of the following four bars:
  * **T < response time lower bound value in ms:** The amount of requests completed under the given Response Time Lower Bound.
  * **response time lower bound in ms < T < response time higher bound value in ms:** The amount of requests completed between the given Response Time Lower Bound and the Response Time Higher Bound.
  * **T > response time higher bound value in ms:** The amount of requests completed over the given Response Time higher Bound.
  * **Failed:** The amount of requests that failed to be completed. 
  <aside class="notice"> To know more about the Response Time Bounds parameters in the Last Run chart, see the [Charts and Indicators](#charts-and-indicators) section.
  </aside>

* **Last 20 Runs:** The last 20 Runs chart shows the results obtained in the Health Checks of  the last 20 Test Case runs. 
  * [Average Response Time](#average-response-time)
  * [Succeeded vs Failed](#succeeded-vs-failed)
  * [Best to Worst Response Time](#worst-response-time)
  <aside class="notice">  To know more about the Last 20 Runs charts, see the [Health Checks](#health-checks) section.
  </aside>



The following image shows the Last Run and Last 20 Runs charts in the Insight section:

<img src="images/test-case-insights.png" alt="Test Case Insights screenshot">

To learn how to interpret the reports that Jeeves creates after each Test Case run, see the [Test Cases Reports](#test-case-reports) section.

<a href="#test-case-home-page" class="go-back"> ← Go back to the Test Case Home Page section </a>



### Test Case Reports
Jeeves is built over the [Gatling Open Source Library](https://gatling.io/docs/current/). Gatling helps Jeeves to create the reports and charts to show the results obtained during the Test Case Run. When a Test Case run is completed, Jeeves and Gatling create a report page, you can check all the reports from previous Test Case Runs ordered by date in the **Reports** section of a Test Case Home page. 

To open a **Report**, follow these steps:

1. Go to the **Reports** section at the bottom of the Test Case Home page.
2. Select the Test Case run date you want to check the report.
3. Click on the link with the date you selected.  
    The link redirects you to the **Gatling Stats** Page. 

The Gatling Stats page contains the statistics, charts, and indicators obtained during a Test Case run. The Gatling Stats page presents the results in two sections:

* **Global:** Shows the overall results of the Test Case run.
* **Details:** Shows the specific results of a Test Case run, useful for when Test Cases test multiple Target URLs or contain other types of [Dynamic Run Parameters](#dynamic-run-parameters).

Both sections of the Gatling Stats page can contain the following sections, depending of the results obtained during the run:

* [Indicators](#indicators)
* [Number of Request](#number-of-request)
* [Statistics](#statistics)
* [Erros](#erros)
* [Active Users along the Simulation](#active-users-along-the-simulation)
* [Response Time Distribution](#response-time-distribution)
* [Response Time Percentiles over Time (OK)](#response-time-percentiles-ocer-time-ok)
* [Number of Request per Second](#number-of-requests-per-second)
* [Number of Responses per Second](#number-of-responses-per-second)
* [Response Time against RPS](#response-time-againts-rps)

To understand the information and charts each one shows, see the next sections.

<a href="#test-case-home-page" class="go-back"> ← Go back to the Test Case Home Page section </a>



#### Indicators
The **Indicators** chart shows the amount of requests that succeeded and failed, based on the fixed values of Response Bound Time(**t**). 
To edit the Response Bound Time values, see the [Charts and Indicators](#charts-and-indicators) section.

The Indicators section shows the information in two graphs:

* **Bar Chart:** Shows the Number of request the Test Case did, organized in the following four bars: 
  * **T < response time lower bound in ms:** The amount of requests completed under the given Response Time Lower Bound.
  * **response time lower bound in ms < T < response time higher bound in ms:** The amount of requests completed between the given Response Time Lower Bound and the Response Time Higher Bound.
  * **T > response time higher bound in ms:** The amount of requests completed over the given Response Time higher Bound.
  * **Failed:** The amount of requests that failed to be completed. 
* **Pie Chart:** Shows the percentage of Succeeded and Failed requests the Test Case did. 

The following image shows an example of an Indicators chart: 

<img src="images/indicators.png" alt="Indicators Reports chart">

To learn how to interpret the information and chart about Number of Request, see the [Number of Request](#number-of-request) section.

<a href="#test-case-reports" class="go-back"> ← Go back to the Test Case Reports section </a>



#### Number of Request
The **Number of Request** chart shows the information about the number of requests that succeeded or failed during the Test Case. The chart represents the information using the following colors:
* **Blue Circles:** The amount of requests that succeeded to get a response.
* **Red Circles:** The amount of requests that failed to get a response.

The following image shows an example of a Number of Request chart:

<img src="images/number-of-request.png" alt="Number of Request Reports chart">

To learn how to interpret the statistics results of the Test Case run, see the [Statistics](#statistics) section.

<a href="#test-case-reports" class="go-back"> ← Go back to the Test Case Reports section </a>



#### Statistics
The **Statistics** table shows the results obtained in the Test Case run. The table shows the information organized in the following columns:

* **Request:**  The name of the request the information in the row belongs to.
* **Executions:** The specific information about the number of requests that succeeded and failed. The Executions column is divided into the following columns:
  * **Total:** The total number of requests, succeeded and failed, sent to the Target URL.
  * **OK:** The total number of requests that succeeded to obtain a response.
  * **KO:** The total number of requests that failed to obtain a response.
  * **%KO:** The percentage of requests that failed to obtain a response.
  * **Cnt/s:** The number of requests in a second.
* **Response Time (ms):** The specific information about the response time that requests took to complete. The Response Time **(ms)** column is divided into the following columns:
  * **Min:** The minimum amount of requests served during the Simulation Work time.
  * **50th Pct:** The amount of requests served when the Simulation Work is at 50% progress time.
  * **75th Pct:** The amount of requests served when the Simulation Work is at 75% progress time.
  * **95th Pct:** The amount of requests served when the Simulation Work is at 95% progress time.
  * **99th Pct:** The amount of requests served when the Simulation Work is at 99% progress time.
  * **Max:** The maximum amount of requests served during the Simulation Work time.
  * **Mean:** The average time a request takes to complete.
  * **Std Dev:** The amount of dispersion of values from the mean response time.

The following image shows an example of the Statistics table: 

<img src="images/statistics.png" alt="Statistics Reports table">

To learn how to interpret the errors obtained during the Test Case run, see the [Errors](#errors) section.

<a href="#test-case-reports" class="go-back"> ← Go back to the Test Case Reports section </a>



#### Errors
The **Errors** table lists the errors obtained during the Test Case run. The table lists the information organized in the following columns: 

* **Errors:** The Error code obtained in the run.
* **Count:** The amount of times the error occurred.
* **Percentage:** The percentage the error occurred, compared to other errors.

The following image is an example of the Errors table:

<img src="images/errors.png" alt="Errors Reports table">

To learn how to interpret the Active Users along the Simulation chart and information of the Test Case run, see the [Active Users along the Simulation](#active-users-along-the-simulation) section.

<a href="#test-case-reports" class="go-back"> ← Go back to the Test Case Reports section </a>



#### Active Users along the Simulation
The **Active Users along the Simulation** chart shows the amount of testing users that were active during the simulation run. The Active Users along the Simulation chart contains the following information:

* **Blue Line:** The name of the Test Case Preset.
* **Yellow line:** The number of active users during the simulation
* **Number of Active Users:** The number of active users in the simulation, represented by the Y axis.
* **Time:** The timestamp when the user was active, represented by the X axis.

The following image is an example of the Active Users along the Simulation chart:

<img src="images/active-users-along-the-simulation.png" alt="Active Users along the Simulation Reports Chart">

To learn how to interpret the Response Time Percentiles over Time chart and information, see the [Response Time Distribution](#response-time-distribution) section.

<a href="#test-case-reports" class="go-back"> ← Go back to the Test Case Reports section </a>



#### Response Time Distribution
The **Response Time Distribution** chart shows the percentage of successful and failed requests grouped by the time it took them to complete. The Response Time Distribution chart contains the following information: 

* **Blue Bar:** The percentage of successful requests in a time group.
* **Red Bar:** The percentage of failed requests in a time group.
* **Percentage of Request:** The percentage of request of each bar, represented by the Y axis.
* **Time:**  The time groups in which the requests completed, represented by the X axis.

The following image is an example of the Response Time Distribution chart:

<img src="images/response-time-distribution.png" alt="Response Time Distribution Reports Chart">

To learn how to interpret the Response Time Distribution chart and information, see the [Response Time Percentiles over Time (OK)](#response-time-percentiles-over-time-ok) section.

<a href="#test-case-reports" class="go-back"> ← Go back to the Test Case Reports section </a>



#### Response Time Percentiles over Time (OK)
The **Response Time Percentiles over Time (OK)** shows a variety of response time percentiles over time, but only for successful requests. The Response Time Percentiles over Time (OK) chart contains the following information:

* **Yellow Line:** The amount of Active Users.
* **Color Spikes:** The Percentile of response times. Each color represents a different percentile.
* **Response Time (ms):** The time response time, represented by the Y axis.
* **Time:** The timestamp of the request response, represented by the X axis.

The following image is an example of the Response Time Percentiles over Time (OK) chart:

<img src="images/response-time-percentiles-over-time.png" alt="Response Time Percentiles over Time(OK) Reports chart">

To learn how to interpret the Number of Requests per Second chart and information, see the [Number of Requests per Second](#number-of-requests-per-second) section.

<a href="#test-case-reports" class="go-back"> ← Go back to the Test Case Reports section </a>



#### Number of Requests per Second
The **Number of Requests per Second** chart shows the amount of requests sent per second over time. The Number of Request per Second chart contains the following information:

* **Yellow Line:** The amount of Active Users.
* **Number of Requests:** The amount of requests sent, represented by the Y axis.
* **Time:**  The time frame of the Test Case run, represented by the X axis.

The following image is an example of the Number Request per Second chart:

<img src="images/number-of-request-per-second.png" alt="Number of Requests per Second Reports chart">

To learn how to interpret the Number of Responses per Second chart and information, see the [Number of Responses per Second](#number-of-responses-per-second) section.

<a href="#test-case-reports" class="go-back"> ← Go back to the Test Case Reports section </a>



#### Number of Responses per Second
The **Number of Responses per Second** chart shows the amount of responses received per second over time, regardless of their status. The Number of Request per Second chart contains the following information:

* **Yellow Line:** The amount of Active Users.
* **Green Spikes:** The amount of Succeeded Responses (OK).
* **Red Spikes:** The amount of Failed Responses (KO).
* **Number of Responses:** The amount of requests sent, represented by the Y axis.
* **Time:** The time frame of the Test Case run, represented by the X axis.

The following image is an example of the Number of Responses per Second chart:

<img src="images/number-of-responses-per-second.png" alt="Number of Responses per Second Reports chart">

To learn how to interpret the Response Time against Global RPS chart and information, see the [Response Time against Global RPS](#response-time-against-global-rps) section.

<a href="#test-case-reports" class="go-back"> ← Go back to the Test Case Reports section </a>



#### Response Time against Global RPS
The **Response Time against Global RPS** chart shows how the response time for the selected request is distributed, based on the amount of requests at the same time. The Response Time against Global RPS chart contains the following information:

* **Blue Dots:** Succeeded requests.
* **Red Dots:** Failed Requests.
* **Response Time:** The overall response time, represented by the Y axis.
* **Global Number of Requests per Second:** The amount of requests that got a response in the given Response Time, represented by the X axis.

The following image is an example of the Responses Time against Global RPS chart:

<img src="images/response-time-againts-global-rps.png" alt="Response Time againts Global RPS Reports chart">

To know how to run multiple Test Cases sequentially, see the [Jobs](#jobs) section. 

<a href="#test-case-reports" class="go-back"> ← Go back to the Test Case Reports section </a>